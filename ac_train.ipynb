{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from RL import ActorCriticAgent\n",
    "from single_agent_envs import SinglePlayerFootballParallel, ACTION_SPACE_SIZE, STATE_SPACE_SIZE\n",
    "torch.manual_seed(3407)\n",
    "torch.cuda.manual_seed(3407)\n",
    "np.random.seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, observation_size, action_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(observation_size, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, action_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, observation_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(observation_size, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COUNT = 4\n",
    "TRAIN_ID = f\"AC_fixed_ball_scratch_norm_parallel_{ENV_COUNT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActorCriticAgent(STATE_SPACE_SIZE, ACTION_SPACE_SIZE, device=\"cuda:1\")\n",
    "agent.create_model(Actor, Critic, actor_lr=0.0003, critic_lr=0.0003, gamma=0.99, entropy_coef=0.0003, gae_lambda=0.9, env_count=ENV_COUNT, step_count=300, reward_norm_factor=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = SinglePlayerFootballParallel(env_count=ENV_COUNT, title=TRAIN_ID, random_ball=False)\n",
    "best_score = 200\n",
    "while envs.running:\n",
    "    states = envs.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = agent.policy(states)\n",
    "        n_states, rewards, dones = envs.step(actions)\n",
    "        agent.learn(states, actions, n_states, rewards, dones)\n",
    "        states = n_states\n",
    "        done = any(dones)\n",
    "    if agent.episode_counter % 100 == 0:\n",
    "        avg = np.mean(agent.reward_history[len(agent.reward_history) - 100:])\n",
    "        if avg > best_score:\n",
    "            best_score = avg\n",
    "            model_scripted = torch.jit.script(agent.actor)\n",
    "            model_scripted.save(f\"best_ac_models/actor_{TRAIN_ID}_{agent.episode_counter}_{round(best_score, 6)}.pt\")\n",
    "            model_scripted = torch.jit.script(agent.critic)\n",
    "            model_scripted.save(f\"best_ac_models/critic_{TRAIN_ID}_{agent.episode_counter}_{round(best_score, 6)}.pt\")\n",
    "del envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{TRAIN_ID}_rewards.txt', 'w') as f:\n",
    "    f.writelines([f\"{round(item, 6)}\\n\" for item in agent.reward_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.training = False\n",
    "env = SinglePlayerFootballParallel(title=TRAIN_ID)\n",
    "for _ in range(10):\n",
    "    s = env.reset()\n",
    "    while not env.loop_once():\n",
    "        s, _, _ = env.step(agent.policy(s))\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor = torch.jit.load(\"best_ac_models/actor_AC_fixed_ball_norm_transfer_it3_4300_171.18.pt\")\n",
    "agent.critic = torch.jit.load(\"best_ac_models/critic_AC_fixed_ball_norm_transfer_it3_4300_171.18.pt\")\n",
    "agent.actor.to(agent.device)\n",
    "agent.critic.to(agent.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(agent.actor)\n",
    "model_scripted.save(f\"best_ac_models/actor/{TRAIN_ID}_{agent.episode_counter}.pt\")\n",
    "model_scripted = torch.jit.script(agent.critic)\n",
    "model_scripted.save(f\"best_ac_models/critic/{TRAIN_ID}_{agent.episode_counter}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
